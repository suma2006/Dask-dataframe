[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Quarto Blog",
    "section": "",
    "text": "Simple Visualisation\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nWelcome Post\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nHello world, this is my first blog post.\nI can write in markdown\nprint(\"Hello World\")\nI can also write math equations:\n\\[\ny = x^2\n\\]\nI can create lists easily:\n\nOne\nTwo\n\nI can also create numbered lists:\n\nOne\nTwo\n\nOr, create a table:\n\n\n\nName\nAge\n\n\n\n\nAlice\n20\n\n\nBob\n21"
  },
  {
    "objectID": "posts/visualisation.html",
    "href": "posts/visualisation.html",
    "title": "Simple Visualisation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Dask dataframe is an extension of pandas which deals with the datasets that are too large to fit into memory. Partitioning these into smaller chunks allows Dask to perform scalable and efficient large-scale data processing using parallel computing.\n\n\n\nInstall Dask using below command\npip install dask[complete]\nTo install Dask using Conda\nconda install -c conda-forge dask\nTo check the version\nimport dask\nprint(dask._version_)\n\n\n\nA Parallel Computing Library for Large Datasets Dask DataFrame fully extends Pandas to skillfully manage substantially large datasets, through parallel and distributed computing. It works well for big data analytics because it scales easily. Here are its key features:\n\n\nDask splits data into smaller parts, processing them in parallel across many CPU cores. It can also powerfully scale the distributed clusters in situations where workloads are substantially larger.\n\n\n\nDask processes each bit of data in multiple chunks, enabling it to work with many terabyte-scale datasets, whereas Pandas loads each bit into memory.\n\n\n\nIt can read and write data from CSV, Parquet, and SQL databases. It also works with AWS S3, GCP, and Azure cloud storage.\n\n\n\nDask optimizes memory use and speed by delaying all computations until the .compute()command is called.\n\n\n\nIt offers a task scheduler that is complete and optimized, along with real-time visual dashboards made for full performance monitoring.\n\n\n\nIt works with Scikit-learn, XGBoost, CuDF (GPU acceleration), and Spark for advanced analytics."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Dask dataframe is an extension of pandas which deals with the datasets that are too large to fit into memory. Partitioning these into smaller chunks allows Dask to perform scalable and efficient large-scale data processing using parallel computing."
  },
  {
    "objectID": "index.html#installation-and-settings",
    "href": "index.html#installation-and-settings",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Install Dask using below command\npip install dask[complete]\nTo install Dask using Conda\nconda install -c conda-forge dask\nTo check the version\nimport dask\nprint(dask._version_)"
  },
  {
    "objectID": "index.html#key-features-and-explanation",
    "href": "index.html#key-features-and-explanation",
    "title": "Dask DataFrame",
    "section": "",
    "text": "A Parallel Computing Library for Large Datasets Dask DataFrame fully extends Pandas to skillfully manage substantially large datasets, through parallel and distributed computing. It works well for big data analytics because it scales easily. Here are its key features:\n\n\nDask splits data into smaller parts, processing them in parallel across many CPU cores. It can also powerfully scale the distributed clusters in situations where workloads are substantially larger.\n\n\n\nDask processes each bit of data in multiple chunks, enabling it to work with many terabyte-scale datasets, whereas Pandas loads each bit into memory.\n\n\n\nIt can read and write data from CSV, Parquet, and SQL databases. It also works with AWS S3, GCP, and Azure cloud storage.\n\n\n\nDask optimizes memory use and speed by delaying all computations until the .compute()command is called.\n\n\n\nIt offers a task scheduler that is complete and optimized, along with real-time visual dashboards made for full performance monitoring.\n\n\n\nIt works with Scikit-learn, XGBoost, CuDF (GPU acceleration), and Spark for advanced analytics."
  },
  {
    "objectID": "index.html#load-the-data-and-check-basic-info",
    "href": "index.html#load-the-data-and-check-basic-info",
    "title": "Dask DataFrame",
    "section": "Load the Data and Check Basic Info",
    "text": "Load the Data and Check Basic Info\nimport dask.dataframe as dd\n# this imports the Dask DataFrame module\nfile_path = '1000000_Sales_Records.csv' \ndf = dd.read_csv(file_path)\n# dd.read_csv(file_path) reads the CSV file lazily\nprint(df.dtypes) \n# loads the entire dataset into memory immediately.\nprint(df.shape)\n#only loads the necessary parts on demand."
  },
  {
    "objectID": "index.html#descriptive-statistics",
    "href": "index.html#descriptive-statistics",
    "title": "Dask DataFrame",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\ndesc_stats = df.describe().compute()\nprint(desc_stats)\n\nprint(\"Total Revenue Sum:\", df['Total Revenue'].sum().compute())\nprint(\"Max Profit:\", df['Total Profit'].max().compute())\nprint(\"Mean Units Sold:\", df['Units Sold'].mean().compute())"
  },
  {
    "objectID": "index.html#sorting-data",
    "href": "index.html#sorting-data",
    "title": "Dask DataFrame",
    "section": "Sorting data",
    "text": "Sorting data\nsorted_df = df.compute().sort_values('Total Profit', ascending=False)\nprint(sorted_df.head())"
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "Dask DataFrame",
    "section": "Use cases",
    "text": "Use cases\n\nHandling large data:\nIf data is too large, Dask helps to break data into small parts and manage them efficiently. ####Faster data analysis: Dask can run multiple tasks at once, accelerating the data analysis process. ### Machine learning on large data: Dask is used to clean and preprocess big data prior to applying it to train machine learning models. ### Real time data processing: Dask ensures data processing is smooth and efficient when dealing with large databases and files. ### Optimizing Data Workflows: Dask handles live data, such as prices of stocks or sensor data, efficiently and rapidly.so that we utilize Dask."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Dask DataFrame",
    "section": "Conclusion",
    "text": "Conclusion\nDask DataFrame is a powerful tool for efficient handling of large datasets. It is an extension of Pandas that allows parallel and distributed computing, enabling you to process terabyte-scale data without overloading memory. It supports several file formats, integrates with big data & machine learning tools, and offers real-time monitoring for optimal performance. Final Thought: When youâ€™re dealing with large-scale data processing Dask is a scalable, efficient, and easy-to-use tool that accelerates computations while saving memory."
  }
]