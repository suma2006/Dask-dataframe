[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Quarto Blog",
    "section": "",
    "text": "Welcome\n\n\n\n\n\nWelcome Post\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Visualisation\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nHello world, this is my first blog post.\nI can write in markdown\nprint(\"Hello World\")\nI can also write math equations:\n\\[\ny = x^2\n\\]\nI can create lists easily:\n\nOne\nTwo\n\nI can also create numbered lists:\n\nOne\nTwo\n\nOr, create a table:\n\n\n\nName\nAge\n\n\n\n\nAlice\n20\n\n\nBob\n21"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Dask dataframe is an extension of pandas which deals with the datasets that are too large to fit into memory. Partitioning these into smaller chunks allows Dask to perform scalable and efficient large-scale data processing using parallel computing.\n\n\n\nInstall Dask using below command\npip install dask[complete]\n\n\n\nscreenshot\n\n\nTo check the version\nimport dask\nprint(dask._version_)\n\n\n\nA Parallel Computing Library for Large Datasets Dask DataFrame fully extends Pandas to skillfully manage substantially large datasets, through parallel and distributed computing. It works well for big data analytics because it scales easily. Here are its key features:\n\n\nDask splits data into smaller parts, processing them in parallel across many CPU cores. It can also powerfully scale the distributed clusters in situations where workloads are substantially larger.\n\n\n\nDask processes each bit of data in multiple chunks, enabling it to work with many terabyte-scale datasets, whereas Pandas loads each bit into memory.\n\n\n\nIt can read and write data from CSV, Parquet, and SQL databases. It also works with AWS S3, GCP, and Azure cloud storage.\n\n\n\nDask optimizes memory use and speed by delaying all computations until the .compute()command is called.\n\n\n\nIt offers a task scheduler that is complete and optimized, along with real-time visual dashboards made for full performance monitoring.\n\n\n\nIt works with Scikit-learn, XGBoost, CuDF (GPU acceleration), and Spark for advanced analytics."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Dask dataframe is an extension of pandas which deals with the datasets that are too large to fit into memory. Partitioning these into smaller chunks allows Dask to perform scalable and efficient large-scale data processing using parallel computing."
  },
  {
    "objectID": "index.html#installation-and-settings",
    "href": "index.html#installation-and-settings",
    "title": "Dask DataFrame",
    "section": "",
    "text": "Install Dask using below command\npip install dask[complete]\n\n\n\nscreenshot\n\n\nTo check the version\nimport dask\nprint(dask._version_)"
  },
  {
    "objectID": "index.html#key-features-and-explanation",
    "href": "index.html#key-features-and-explanation",
    "title": "Dask DataFrame",
    "section": "",
    "text": "A Parallel Computing Library for Large Datasets Dask DataFrame fully extends Pandas to skillfully manage substantially large datasets, through parallel and distributed computing. It works well for big data analytics because it scales easily. Here are its key features:\n\n\nDask splits data into smaller parts, processing them in parallel across many CPU cores. It can also powerfully scale the distributed clusters in situations where workloads are substantially larger.\n\n\n\nDask processes each bit of data in multiple chunks, enabling it to work with many terabyte-scale datasets, whereas Pandas loads each bit into memory.\n\n\n\nIt can read and write data from CSV, Parquet, and SQL databases. It also works with AWS S3, GCP, and Azure cloud storage.\n\n\n\nDask optimizes memory use and speed by delaying all computations until the .compute()command is called.\n\n\n\nIt offers a task scheduler that is complete and optimized, along with real-time visual dashboards made for full performance monitoring.\n\n\n\nIt works with Scikit-learn, XGBoost, CuDF (GPU acceleration), and Spark for advanced analytics."
  },
  {
    "objectID": "index.html#load-the-data-and-check-basic-info",
    "href": "index.html#load-the-data-and-check-basic-info",
    "title": "Dask DataFrame",
    "section": "Load the Data and Check Basic Info",
    "text": "Load the Data and Check Basic Info\nimport dask.dataframe as dd\n# this imports the Dask DataFrame module\nfile_path = '1000000_Sales_Records.csv' \ndf = dd.read_csv(file_path)\n# dd.read_csv(file_path) reads the CSV file lazily\nprint(df.dtypes) \n# loads the entire dataset into memory immediately.\nprint(df.shape)\n#only loads the necessary parts on demand.\n\n\n\nscreenshot"
  },
  {
    "objectID": "index.html#descriptive-statistics",
    "href": "index.html#descriptive-statistics",
    "title": "Dask DataFrame",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\ndesc_stats = df.describe().compute()\n#Computes summary statistics (like mean, max, min, etc.) for numeric columns and converts the lazy Dask computation into an actual Pandas DataFrame.\nprint(desc_stats)\n# prints the result.\nprint(\"Total Revenue Sum:\", df['Total Revenue'].sum().compute())\n# Computes the total sum of the \"Total Revenue\" column and prints the total revenue.\nprint(\"Max Profit:\", df['Total Profit'].max().compute())\n# Finds the maximum value in the \"Total Profit\" column, Executes and retrieves the highest profit value.\nprint(\"Mean Units Sold:\", df['Units Sold'].mean().compute())\n# Computes the average (mean) number of units sold and Executes the operation and prints the final numeric value."
  },
  {
    "objectID": "index.html#sorting-data",
    "href": "index.html#sorting-data",
    "title": "Dask DataFrame",
    "section": "Sorting data",
    "text": "Sorting data\nsorted_df = df.compute().sort_values('Total Profit', ascending=False)\n# Sorts the DataFrame by \"Total Profit\" in descending order (highest profit first)\nprint(sorted_df.head())\n# Prints the first 5 rows of the sorted DataFrame.\n\n\n\nscreenshot"
  },
  {
    "objectID": "index.html#handeling-missing-data",
    "href": "index.html#handeling-missing-data",
    "title": "Dask DataFrame",
    "section": "Handeling Missing data",
    "text": "Handeling Missing data\ndf_cleaned = df.dropna()\n# Compute the result (Dask is lazy, so you need to call compute to get the actual DataFrame)\ndf_cleaned_computed = df_cleaned.compute()\n# Show the cleaned data\nprint(df_cleaned_computed)"
  },
  {
    "objectID": "index.html#aggregation-and-grouping",
    "href": "index.html#aggregation-and-grouping",
    "title": "Dask DataFrame",
    "section": "Aggregation and grouping",
    "text": "Aggregation and grouping\ndf_grouped=df.groupby('Item Type',observed=True)['Total Profit']\n# Groups the DataFrame based on unique values \nprint('Multiple aggregations:')\ndisplay(df_grouped.agg(['count','sum','mean','std']))\n# Applies multiple aggregations (count, sum, mean, std) on \"Total Profit\" and displays the results."
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "Dask DataFrame",
    "section": "Use cases",
    "text": "Use cases\n\nHandling large data:\nIf data is too large, Dask helps to break data into small parts and manage them efficiently.\n\n\nFaster data analysis:\nDask can run multiple tasks at once, accelerating the data analysis process.\n\n\nMachine learning on large data:\nDask is used to clean and preprocess big data prior to applying it to train machine learning models.\n\n\nReal time data processing:\nDask ensures data processing is smooth and efficient when dealing with large databases and files.\n\n\nOptimizing Data Workflows:\nDask handles live data, such as prices of stocks or sensor data, efficiently and rapidly.so that we utilize Dask."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Dask DataFrame",
    "section": "Conclusion",
    "text": "Conclusion\nDask DataFrame is a powerful tool for efficient handling of large datasets. It is an extension of Pandas that allows parallel and distributed computing, enabling you to process terabyte-scale data without overloading memory. It supports several file formats, integrates with big data & machine learning tools, and offers real-time monitoring for optimal performance. Final Thought: When youâ€™re dealing with large-scale data processing Dask is a scalable, efficient, and easy-to-use tool that accelerates computations while saving memory."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Dask DataFrame",
    "section": "References",
    "text": "References\nDask official Documentation - https://docs.dask.org/en/stable/index.html Dask Tutorials"
  },
  {
    "objectID": "posts/visualisation.html",
    "href": "posts/visualisation.html",
    "title": "Simple Visualisation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)"
  }
]